{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP: Artfical neural networks\n",
    "## Student: CÃ©sar Almecija\n",
    "# ===========================================================\n",
    "\n",
    "The objective of this practical session is to implement from scratch an artificial neuron network architecture using the numpy library, then to train this neuron network in order to learn the function (non-linear):\n",
    "\n",
    "$$\n",
    "f^*(x_1, x_2) = x_1^2 + x_2^2\n",
    "$$\n",
    "\n",
    "\n",
    "from training data:\n",
    "\n",
    "$$\n",
    "\\{(x_n, y_n) \\in \\mathbb{R}^2 \\times \\mathbb{R}, n = 1, \\dots, N \\},\n",
    "$$\n",
    "\n",
    "for which:\n",
    "\n",
    "$$\n",
    "\\forall n \\in \\{1, \\dots, N \\}, y_n = f^*(x_n) + \\epsilon_n,\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\epsilon_n$ is Gaussian white noise with zero mean and variance $\\sigma^2$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generation of training and test data\n",
    "\n",
    "The function below is used to generate data from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(nsamples, sigma=0.):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate a dataset with the specified number of samples\n",
    "    \n",
    "    :param nsamples: number of sample points\n",
    "    :type nsamples: int\n",
    "    :param sigma: standard deviation of the noise\n",
    "    :type sigma: float\n",
    "    \n",
    "    :return: Generated dataset {(x_n, y_n), n = 1, ..., nsamples}\n",
    "    :rtype: tuple of numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.zeros((nsamples, 2))\n",
    "    x[:, 0] = np.random.uniform(-1, 1, nsamples)\n",
    "    x[:, 1] = np.random.uniform(-1, 1, nsamples)\n",
    "\n",
    "    eps = np.random.normal(loc=0, scale=sigma, size=nsamples)\n",
    "    \n",
    "    y = x[:, 0]**2 + x[:, 1]**2 + eps\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 50\n",
    "sigma = 0.\n",
    "xtrain, ytrain = generate_dataset(nsamples, sigma)\n",
    "xtest, ytest = generate_dataset(nsamples, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing a network layer\n",
    "\n",
    "We will rely on a class, Layer, whose attributes are:\n",
    "\n",
    "- the \"input_dim\" size of the signal at the input of the layer\n",
    "- the number \"output_dim\" of neurons in the layer\n",
    "- the activation function \"activation\" used by the layer ('RELU' or none)\n",
    "- the \"weights\" matrix of the weights of the neurons of the layer\n",
    "- the \"biases\" vector of the biases\n",
    "- the input signal \"input\"\n",
    "- the output signal \"output\"\n",
    "- the gradients \"input_grad\", \"output_grad\", \"weights_grad\", and \"biases_grad\"\n",
    "\n",
    "This class allows us to implement what takes place inside a layer of the neural network. \n",
    "\n",
    "Complete the implementation of the class:\n",
    "\n",
    "**Question 2.1.** In the constructor, initialize the values of the weight and bias matrix.\n",
    "\n",
    "**Question 2.2.** Implement the computation of the output signal y of the layer in the presence of an input x. We remind here that:\n",
    "\n",
    "$$\n",
    "y = \\sigma (Wx) + b,\n",
    "$$\n",
    "\n",
    "where $\\sigma $ is the activation function of the layer and $b$ the bias.\n",
    "\n",
    "**Question 2.3.** Implement the backpropagation algorithm of the gradient of the output signal \"output_grad\" in the layer. For a layer $i$, we have:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "t_i &=& b_i + W_i x \\\\\n",
    "y_i &=& g_i(t_i) \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "It can really help you if you calculate the derivative $\\frac{\\partial t}{\\partial W}$. Think about how to write this mathematically prior of implementing it. \n",
    "\n",
    "\n",
    "\n",
    "**Question 2.4.** Implement a method to update the weight / bias values of the layer knowing the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just init the activation functions\n",
    "@np.vectorize\n",
    "def ReLU(inp):\n",
    "    if inp < 0:\n",
    "        return 0\n",
    "    return inp\n",
    "\n",
    "@np.vectorize\n",
    "def identity(inp):\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    \"\"\"\n",
    "    Neural network layer implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, activation='RELU'):\n",
    "\n",
    "        \"\"\"\n",
    "        :param input_dim: dimension of the input vector\n",
    "        :type input_dim: integer\n",
    "\n",
    "        :param output_dim: dimension of the output vector \n",
    "         (i.e number of neurons in the layer)\n",
    "        :type output_dim: integer\n",
    "\n",
    "        :param activation: activation function\n",
    "        :type activation: string\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.input_dim  = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Biases is a vector of output_dim lines (1 bias per neuron)\n",
    "        self.biases     = np.zeros( (output_dim) )\n",
    "        # Weights is a matrix of output_dim lines and input_dim columns (input_dim weights per neuron)\n",
    "        self.weights    = np.random.normal(loc=0, scale = 2/np.sqrt(input_dim), size = (output_dim, input_dim) )\n",
    "        \n",
    "        # activation function init\n",
    "        if activation == 'RELU':\n",
    "            self.act_function = ReLU\n",
    "        elif activation == None or activation == 'None':\n",
    "            self.act_function = identity\n",
    "        else:\n",
    "            print(\"Unknown activation function, using None instead\")\n",
    "            self.act_function = identity\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes a forward pass in the layer (computes the output and the local derivatives)\n",
    "\n",
    "        :param x: input signal\n",
    "        :type x: numpy array of size input_dim\n",
    "\n",
    "        :return: output of the layer\n",
    "        :rtype: numpy array of size output_dim\n",
    "        \"\"\"\n",
    "        \n",
    "        self.input = x\n",
    "        \n",
    "        # First, we compute the output_dim outputs\n",
    "        output = np.matmul(self.weights, x) + self.biases\n",
    "        output = self.act_function(output)\n",
    "        self.output = output\n",
    "        \n",
    "        # Then, we compute all the \"local derivatives\"\n",
    "        # (ie the partial derivatives of the outputs w.r.t. each bias, weight and input)\n",
    "        # NB: WE CAN ALSO DO THIS STEP IN THE BACKWARD PASS\n",
    "        # ==================================================\n",
    "        # we store them in this way:\n",
    "        # - input_grad  : the partial derivatives of the outputs w.r.t. all the inputs (matrix of size input_dim * output_dim)\n",
    "        # - weights_grad: the partial derivatives of the outputs w.r.t. all the weights (matrix of size input_dim * output_dim)\n",
    "        # - biases_grad : the partial derivatives of the outputs w.r.t. all the biases (vector of size output_dim)\n",
    "        \n",
    "        if self.activation == 'RELU':\n",
    "            pre_activation_positive = np.array ( [ (np.dot(self.input, self.weights[i, :]) + self.biases[i] >= 0) for i in range(output_dim) ] )\n",
    "        else:\n",
    "            pre_activation_positive = np.ones( (output_dim) )\n",
    "        \n",
    "        self.input_grad   = np.multiply(self.weights.T, pre_activation_positive)\n",
    "        \n",
    "        large_x = np.tile( x[:, np.newaxis], self.output_dim)\n",
    "        self.weights_grad = np.multiply(large_x, pre_activation_positive)\n",
    "        \n",
    "        self.biases_grad = pre_activation_positive\n",
    "        \n",
    "        # Finally, we return the output (what we have been waiting for!)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def backward(self, output_grad):\n",
    "        \"\"\"\n",
    "        Computes a backward pass in the layer\n",
    "\n",
    "        :param output_grad: gradient of the output of the layer w.r.t\n",
    "         the training loss\n",
    "        :type output_grad: numpy array of size output_dim\n",
    "\n",
    "        :return: gradient of the input of the layer w.r.t\n",
    "         the training loss\n",
    "        :rtype: numpy array of size input_dim\n",
    "        \"\"\"\n",
    "        \n",
    "        # We have already computed the local gradients in the forward pass\n",
    "        # Now, let's backpropagate. The following variables (used in the forward pass) now store:\n",
    "        # - input_grad  : the partial derivatives of the loss w.r.t. all the inputs\n",
    "        # - weights_grad: the partial derivatives of the loss w.r.t. all the weights\n",
    "        # - biases_grad : the partial derivatives of the loss w.r.t. all the biases\n",
    "\n",
    "        self.output_grad = output_grad\n",
    "        self.weights_grad = np.matmul(self.weights_grad, self.output_grad)\n",
    "        self.biases_grad = np.multiply(self.output_grad, self.biases_grad)\n",
    "        \n",
    "        return np.matmul(self.input_grad, output_grad)\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weights during the gradient descent\n",
    "\n",
    "        :param learning_rate: learning rate\n",
    "        :type learning_rate: float\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update the weights and the biases\n",
    "        \n",
    "        self.weights -= learning_rate * self.weights_grad.T\n",
    "        self.biases  -= learning_rate * self.biases_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear model \n",
    "\n",
    "The Linear class below is used to implement the linear model\n",
    "\n",
    "$$\n",
    "f^*(x_1, x_2) = w_1 x_1 + w_2 x_2\n",
    "$$\n",
    "\n",
    "\n",
    "associated with the cost function corresponding to the standard $L^2$.\n",
    "\n",
    "\n",
    "The implementation is based on the Layer class: the linear model is in fact nothing more than a network layer without an activation function.\n",
    "\n",
    "The attributes of the class are:\n",
    "- the \"input_dim\" and \"output_dim\" dimensions of the input and output signals\n",
    "- the instance of the Layer class used to describe the model\n",
    "- the output signal \"output\"\n",
    "- the \"target\" target used during training\n",
    "- the \"loss\" value of the cost function:\n",
    "$$\n",
    "loss = (output - target)^2\n",
    "$$\n",
    "- the \"loss_grad\" gradient of the cost function compared to the output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "\n",
    "    \"\"\"\n",
    "    Linear model implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim=1):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layer = Layer(self.input_dim, self.output_dim, activation='None')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        Computes a forward pass in the neural network\n",
    "\n",
    "        :param x: input signal\n",
    "        :type x: numpy array of size input_dim\n",
    "\n",
    "        :return: output of the neural network\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        self.output = self.layer.forward(x)\n",
    "\n",
    "\n",
    "    def compute_loss(self, x, target):\n",
    "        \"\"\"\n",
    "        Computes the loss \n",
    "\n",
    "        :param x: input signal\n",
    "        :type x: numpy array of size input_dim\n",
    "\n",
    "        :param target: target value\n",
    "        :type target: float\n",
    "\n",
    "        :return: loss\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "\n",
    "        self.target = target\n",
    "        self.forward(x)\n",
    "        self.loss = (self.output - target)**2\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Backpropagation in the neural network \n",
    "        \"\"\"\n",
    "        self.loss_grad = 2*(self.output - self.target)\n",
    "        self.layer.backward(self.loss_grad)\n",
    "\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weights of the network during the gradient descent\n",
    "\n",
    "        :param learning_rate: learning rate\n",
    "        :type learning_rate: float\n",
    "        \"\"\"\n",
    "        self.layer.update(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network\n",
    "\n",
    "**Question 4.1** Complete the implementation of the TwoLayersNetwork class below, inspiring of the linear class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayersNetwork:\n",
    "\n",
    "    \"\"\"\n",
    "    Linear model implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1, activation=\"None\"):\n",
    "        \"\"\"\n",
    "        Model initialization\n",
    "        \n",
    "        :param input_dim: Dimension of the input signal\n",
    "        :type input_dim: int\n",
    "        \n",
    "        :param hidden_dim: Dimension of the hidden layer\n",
    "        :type input_dim: int\n",
    "        \n",
    "        :param output_dim: Dimension of the output signal\n",
    "        :type output_dim: int\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_layer = Layer(self.input_dim, self.hidden_dim, activation='RELU')\n",
    "        self.output_layer = Layer(self.hidden_dim, self.output_dim, activation='RELU')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes a forward pass in the neural network\n",
    "\n",
    "        :param x: input signal\n",
    "        :type x: numpy array of size input_dim\n",
    "        \"\"\"\n",
    "        self.output_of_hidden = self.hidden_layer.forward(x)\n",
    "        self.output           = self.output_layer.forward(self.output_of_hidden)\n",
    "\n",
    "    def compute_loss(self, x, target):\n",
    "        \"\"\"\n",
    "        Computes the loss \n",
    "\n",
    "        :param x: input signal\n",
    "        :type x: numpy array of size input_dim\n",
    "\n",
    "        :param target: target value\n",
    "        :type target: float\n",
    "        \"\"\"\n",
    "        \n",
    "        self.target = target\n",
    "        self.forward(x)\n",
    "        self.loss = (self.output - target)**2\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Backpropagation in the neural network \n",
    "        \"\"\"\n",
    "        \n",
    "        self.loss_grad = 2*(self.output - self.target)\n",
    "        output_layer_grad = self.output_layer.backward(self.loss_grad)\n",
    "        self.hidden_layer.backward(output_layer_grad)\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weights of the network during the gradient descent\n",
    "\n",
    "        :param learning_rate: learning rate\n",
    "        :type learning_rate: float\n",
    "        \"\"\"\n",
    "        self.output_layer.update(learning_rate)\n",
    "        self.hidden_layer.update(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Linear model training\n",
    "\n",
    "The code below is used to train the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal training error: 0.22883446096559232\n",
      "Minimal test error: 0.1782717009407258\n"
     ]
    }
   ],
   "source": [
    "    # 1: Linear model\n",
    "\n",
    "    input_dim = 2\n",
    "    output_dim = 1\n",
    "    nepochs = 100\n",
    "\n",
    "    # Initializes the model\n",
    "    linear = Linear(input_dim, output_dim)\n",
    "    \n",
    "    # Fix the learning rate\n",
    "    learning_rate = 1e-2\n",
    "\n",
    "    training_loss, test_loss = [], []\n",
    "    for epoch in range(nepochs):\n",
    "        \n",
    "        train_err = []\n",
    "        for n in range(nsamples):\n",
    "\n",
    "            linear.compute_loss(xtrain[n], ytrain[n])\n",
    "            linear.backward()\n",
    "            linear.update(learning_rate)\n",
    "            train_err.append(linear.loss)\n",
    "\n",
    "        test_err = []\n",
    "        for n in range(nsamples):\n",
    "\n",
    "            linear.compute_loss(xtest[n], ytest[n])\n",
    "            test_err.append(linear.loss)\n",
    "\n",
    "        training_loss.append(np.array(train_err).mean())\n",
    "        test_loss.append(np.array(test_err).mean())\n",
    "\n",
    "    plt.plot(np.array(training_loss), label='training loss')\n",
    "    plt.plot(np.array(test_loss), label='test loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Minimal training error: \" + str(min(training_loss)))\n",
    "    print(\"Minimal test error: \" + str(min(test_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do a little test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN value for f(0.5, 0.5):  0.6783401536759943\n",
      "Expected value          :   0.5\n"
     ]
    }
   ],
   "source": [
    "linear.forward(np.array([0.5, 0.5]))\n",
    "print(\"NN value for f(0.5, 0.5): \", linear.output[0])\n",
    "print(\"Expected value          :  \", 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Neural network training\n",
    "\n",
    "**Question 6.1.** Using the linear model as an inspiration, implement the training of the neural network on the data\n",
    "\n",
    "**Question6.2.** What is the influence of the number of neurons in the hidden layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal training error: 0.019915022495245852\n",
      "Minimal test error: 0.03282757344403892\n"
     ]
    }
   ],
   "source": [
    "    # 2: NN\n",
    "\n",
    "    input_dim = 2\n",
    "    hidden_dim = 8\n",
    "    output_dim = 1\n",
    "    nepochs = 100\n",
    "\n",
    "    # Initializes the model\n",
    "    two_layers = TwoLayersNetwork(input_dim, hidden_dim, output_dim)\n",
    "    \n",
    "    # Fix the learning rate\n",
    "    learning_rate = 1e-2\n",
    "\n",
    "    training_loss, test_loss = [], []\n",
    "    for epoch in range(nepochs):\n",
    "        \n",
    "        train_err = []\n",
    "        for n in range(nsamples):\n",
    "\n",
    "            two_layers.compute_loss(xtrain[n], ytrain[n])\n",
    "            two_layers.backward()\n",
    "            two_layers.update(learning_rate)\n",
    "            train_err.append(two_layers.loss)\n",
    "\n",
    "        test_err = []\n",
    "        for n in range(nsamples):\n",
    "\n",
    "            two_layers.compute_loss(xtest[n], ytest[n])\n",
    "            test_err.append(two_layers.loss)\n",
    "\n",
    "        training_loss.append(np.array(train_err).mean())\n",
    "        test_loss.append(np.array(test_err).mean())\n",
    "\n",
    "    plt.plot(np.array(training_loss), label='training loss')\n",
    "    plt.plot(np.array(test_loss), label='test loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Minimal training error: \" + str(min(training_loss)))\n",
    "    print(\"Minimal test error: \" + str(min(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN value for f(0.5, 0.5):  0.5797442543725934\n",
      "Expected value          :  0.5\n"
     ]
    }
   ],
   "source": [
    "two_layers.forward(np.array([0.5, 0.5]))\n",
    "print(\"NN value for f(0.5, 0.5): \", two_layers.output[0])\n",
    "print(\"Expected value          :  0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of hidden neurons allows the model to learn better. However, the risk of overfitting increases when doing so. This is why a compromise must be made, and 8 neurons in the hidden layer seems to be a good option.\n",
    "\n",
    "Furthermore, adding neurons in the hidden layer increases the calculation time. The compromise must also take this into account: increasing significantly the calculation time by adding neurons must meet a significant increase in the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
